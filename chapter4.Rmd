---
title: "chapter4"
author: "Julia ROQUIGNY"
date: "2023-11-27"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

install.packages("dplyr")

```{r cars}
#2 
##load dataset Boston from MASS package 


library(dplyr)
library(MASS)

data <- MASS::Boston

## explore the structure and the dimensions of the dataset 
str(data)
head(data)

```
The data consist of 506 observations of 14 variables. 


install.packages("ggplot2")

```{r}
#3 
 
library(ggplot2)
library(corrplot)

##plot correlation plot 
corrplot.mixed(cor(data, use = "pairwise"), order = "hclust")
cor_matrix <- cor(data) 
cor_matrix

## Graphical overview of the data 

par(mfrow = c(3,3))
hist(data$crim)
hist(data$zn)
hist(data$indus)
hist(data$nox)
hist(data$rm)
hist(data$age)

par(mfrow = c(3,3))

hist(data$dis)
hist(data$tax)
hist(data$ptratio)
hist(data$black)
hist(data$lstat)
hist(data$medv)

##summary
summary(data)


```
Except 'rm' for the average number of rooms per dwelling, all the numerical variables don't look normally distributed. 

We can observe strong correlations  between the variables rad and tax (0.91) dis and age (0.75), dis and nox (0.77), indus and nox (0.76), nox and age (0.73), lstat and medv (0.74).


```{r}
#4

## center and standardize variables
data_scaled <- scale(data)

## print summaries of the scaled variables
summary(data_scaled) # note that now all means are 0 as supposed to 

## change the object to data frame
data_scaled_df <- as.data.frame(data_scaled)

## create a quantile vector of crim
bins <- quantile(data_scaled_df$crim)

# create a categorical variable 'crime'
crime <- cut(data_scaled_df$crim, breaks = bins, include.lowest = TRUE, label = c("low", "med_low", "med_high", "high"))

# remove original crim from the dataset
data_scaled_df <- dplyr::select(data_scaled_df, -crim)

# add the new categorical value to scaled data
data_scaled_df <- data.frame(data_scaled_df, crime)


# number of rows in the Boston dataset 
n <- nrow(data_scaled_df)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train and test set
train <- data_scaled_df[ind,]
test <-data_scaled_df[-ind,]


```

All standardized  values correspond to a mean = 0. 
and create a categorical variable “crime”. Let’s also divide the dataset to train and test sets, so that 80% of the data belongs to the train set.




```{r}
#5 
## fit a linear discriminant analysis
lda.fit <- lda(crime ~ . , data = train)

lda.fit

## the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  graphics::arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results (select both lines and execute them at the same time!)
plot(lda.fit, dimen = 2)
lda.arrows(lda.fit, myscale = 1)



```

```{r}
#6 

## save the crime categories from test data
classes <- test$crime

## remove the crime variable from test data
test <- dplyr::select(test, -crime)

## predict classes with test data
lda_pred <- predict(lda.fit, newdata = test)

## cross tabulate the results
table(correct = classes, predicted = lda_pred$class)

```
The results show there is a relatively small amount of false observations in the model low and med_high. There is no false observations for high. However there is a high amount of false observations for med_low.  
 9/27,14/21, 8/28 and 0/26  are falsely categorized for low, med_low,med_high, and high respectively. 
 
```{r}
#7

library(ggplot2)

## load the data
data <- MASS::Boston

## scale the Boston dataset 
data_scaled <- scale(data)

## calculate euclidean distances
dist_eu <- dist(data)

## look at the summary of the distances
summary(dist_eu)

# set seed to get reproducable results 
set.seed(123)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(data, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line') 

# k-means clustering
km <- kmeans(data_scaled, centers = 2)

# plot the Boston dataset with clusters
pairs(data_scaled, col = km$cluster)

```
 
